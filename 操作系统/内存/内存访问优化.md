# 内存访问优化

## cache line 优化

当硬件监测到连续地址访问模式出现时，会激活多层预取器，将预测将要访问的数据加载到合适的缓存层级当中。这样，当后续访问真实到来的时候，能够从更近的缓存层级中获取到数据，从而加速访问速度

因为 L1 容量有限，L1 的硬件预取步长较短，加速目标主要为了提升 L2 到 L1，而 L2 和 LLC 的预取步长相对较长，用于将主存提升到 cache

## cache line 对齐

为了避免频繁的主存交互，其实缓存体系采用了类似 malloc 的方法，即划分一个最小单元，叫做缓存行，主流 CPU 上一般 64B，所有内存到缓存的操作，以缓存行为单位整块完成

对于连续访问来说第一个 B 的访问就会触发全部 64B 数据都进入 L1，后续的 63B 访问就可以直接由 L1 提供服务

位于不同的两个缓存行的数据，是可以被真正独立加载、淘汰和转移的，因为 cache 间流转的最小单位是一个cache line

因为 cache line 的独立性，带来了 false share 问题，也就是不慎将两个本无竞争的数据放置在一个缓存行内，导致因为体系结构的原因，引入了本不存在的竞争

## 缓存一致性

当多个核心都要修改确实需要在同一个缓存行内的数据（往往就是同一个数据）时，由于在多核心系统中 cache 存在多份，因此就需要考虑这多个副本间一致性的问题，而这个一致性一般由一套状态机协议保证（MESI及其变体）

当竞争写入发生时，需要竞争所有权，未获得所有权的核心，只能等待同步到修改的最新结果之后，才能继续自己的修改