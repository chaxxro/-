# RAG

RAG 全称检索增强生成（Retrieval-Augmented Generation）,是当前 AI 应用中常见的辅助方法，有效提升了 LLM 输出的准确性和可靠性

## LLM 遇到的问题

### 知识截止

LLM 训练不是实时的，而是离线训练好的。在训练过程中，使用的数据都是提前准备的，而且大多数是公开、开源的数据，这就导致了 LLM 训练后具备的知识是有范围的

模型知识仅限于训练数据所涵盖的知识范围，对于新的知识（比如今天的新闻）或未训练的知识（比如未公开的数据），模型本身不具备这些知识，仅具备推理能力

### 幻觉现象

一方面，LLM 是一个条件概率模型，以前文作为条件的词表概率逐词生成文本，这一机制导致其可能出现看似逻辑严谨（概率高）但其实缺乏事实依据的生成，也就是一本正经地胡说八道

另一方面，LLM 的训练过程，是对训练数据的知识进行压缩提炼的过程，但不是无损压缩知识，边缘知识容易在主流知识冲击下出现扭曲，导致产生了幻觉

## RAG

RAG 就是一种能够有效幻觉模型知识截止和幻觉现象的方法，是指对 LLM 输入进行优化，使其能够在生成响应之前引用训练数据来源之外的知识，作为回答的根据

RAG 系统有两个最主要的组成部分：

- 检索 Retrieval：查询外部数据源，常见的检索方法有全文检索、向量检索、图检索等
- 生成 Generation：将检索信息提供给 LLM，生成回答

### Naive RAG

RAG 系统的最基本实现，使用单一的全文检索或向量检索，从文档集合中检索出与 query 相关的文档，直接将检索的文档用于增强 LLM 的生成

- 缺乏语义理解：全文匹配依赖词汇匹配，无法捕捉到 query 与文档之间的语义关联。向量检索受限于间接匹配，语义理解能力也不足
- 输出效果差：由于缺乏对 query、文档的高级预处理、后处理，召回的文档容易包含过多或过少信息，导致最终生成的回答过于宽泛
- 效果优化困难：系统过于依赖单一检索技术，未对 query、文档进行增强，导致优化局限于检索技术

### Advanced RAG

Advanced RA G在 Naive RAG 的基础上，对检索前、检索、检索后三个阶段进行改进

在检索前阶段：

- 增强文档质量，比如优化章节结构、增强标题等，过滤低质量信息
- 优化索引结构，优化 chunk size 使得 context 粒度符合应用场景的需求
- 优化索引信息，对 chunk 进行提取、增强，作为 embedding 文本
- 对用户 query 的进行 rewriting

在检索阶段：

- 使用域内知识对 embedding 进行 fine-tune
- 使用 llm-based embedding 模型，生成对上下文理解更准确的语义向量

在检索后阶段：

- 增加 reranking 提高检索文档的相关性
- 增加 context-compression 使提供给模型的信息更加集中

### Modular RAG

将检索和生成分解为独立可复用的组件，从而实现特定域的优化和任务适应性

Modular RAG 将 RAG 系统所使用到的多种检索、存储、路由等等全部模块化，并且可以根据特定的场景，对这些模块进行重新排列，如多种检索方式的混合检索等，以取得更好的效果

### Graph RAG

使用图结构来扩展传统的 RAG 系统，利用图的关系和层级结构，增强 multi-hop 推理和context 丰富度。Graph RAG 可以生成的结果更丰富更准确，特别是对于需要关系理解的任务

### Agentic RAG

Agentic RAG 使用能够动态决策和工具调用的 LLM-based agent，来解决更加复杂、实时和多域的查询

得助于 LLM-based 的工具调用能力，Agentic RAG 能够使用更多更复杂的工具来辅助检索，比如搜索引擎、计算器等各类以 API 形式能够访问的工具

Agentic RAG可以根据实际的检索场景动态决策